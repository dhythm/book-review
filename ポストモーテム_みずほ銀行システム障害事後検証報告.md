---
marp: true
---

# ポストモーテム

## みずほ銀行　システム障害事後検証報告

---

## はじめに

- IT 業界のポストモーテム（事後検証）は非難の為ではなく経験を教訓とするためにある

---

## 第１章

- トラブルの告知をできていなかったため、ATM にカードを取り込まれる被害者が増え続けた
- 頭取がニュースで障害を知る
- e-口座への切替移行作業を断念、年間 16 億削減の目論見が潰える
- ミスのリカバリで省略したオペレーションで法律違反
- 真因に「言うべきことを言わない、言われたことだけしかしない姿勢」の社風
- 業績を好転させた社長もシステム障害で退任を余儀なくされる

---

## 第２章

- STEPS は富士通のメインフレームでシングルベンダー体制
- MINORI は SOA でマルチベンダー体制

---

### ATM 障害（最初の障害）

- DB のインデックスファイルの保存仕様変更も、運用担当部門に課題意識なし
- MINORI 移行リハーサル時にあった類似トラブルに対して課題意識なし
- MINORI 稼働後の類似トラブルも、仕様変更検討ならびに経営陣への報告なし
- テストでの処理件数不足によるバグの見逃し
- エラー監視を行う完全子会社に十分な体制なし
- 結果、閾値を超えるトラブルの兆候を見逃した
- データセンターオペレーターから担当者に連絡するも、担当者は詳細確認・ログの確認をせず

---

- コールセンター業務は再々委託
- 障害発生後、役員級への報告までに 1 時間以上かかった
- 金融障害の原因を誤認し、ガードレールを解除してしまう
- 担当部署が障害ランクを過小評価
- 運用担当の会社にはリモートでログを調査する仕組みがなかったため、出社する必要があった

---

### ３回目の障害

- 追加したプログラムにミス
- 同プログラムの設計開発を MHRT に委託
- MHRT は作業を IT ベンダーに再委託
- プログラムは IT ベンダーにてレビュー、レビュー結果を MHRT が確認する体制
- みずほも MHRT も形式的な確認にとどまり、プログラムミスを見過ごした

---

### ４回目の障害

- 先行ジョブが異常終していたことを見逃した
- IT・システム統括第一部は、対象のジョブがゼロ件実行で実行済となっていたため問題なしと判断した
- 手動実行する方針に切り替えたが、集めた情報が足りず、打鍵入力できなかった

---

### ５回目の障害

- DB サーバーのディスク装置が故障
- ミラーリングディスクにコピーを開始したが、コピー中にディスクが故障
- DB サーバーを冗長構成にしていたが、システムがデータの最新性を保証できないと判断し、切り替えが失敗した
- 災害対策用のサブセンターへの切り替えを試みたが、勘定系の切替手順書しか存在しなかった

---

### ８回目の障害

- 送金の電文が遅延
- AML システムによるチェックを省略しても法令違反にならないと経営層が判断
- しかし外為法違反だった

---

### ９回目の障害

- ルールを誤認し、切替時刻を間違った
- IT 部門の担当者が時刻設定を誤った
- 業務部門の担当者は設定作業がなされた事実だけを確認した
- 2018-2020 年は、モアタイム導入した担当者がチェックしていたが、交代で引き継ぎがなされていなかった

---

### １０回目の障害

- ストアドプロシージャが無効になり、処理に時間がかかった
- 実行計画の最適化を見直た際に発生したトラブルが原因

---

## なぜ障害は拡大したのか

- 他行では 1990 年代に改めた仕様が残存
- アプリのエラー設計に問題あり
- 原因が特定できない状態で禁止条件を緩和する運用ミス
- 24 時間 365 日で監視する体制を構築できていなかった
- トラブル発生時の伝達プロセスがレガシーで、双方の記録にも差分がある
- 障害ランクの過小見積 - 障害の規模を人が見積もる（ルール化されていない）
- 障害に対する会議を始めるのに 4 時間 40 分かかったケースもある
- 対外告知までに 3 時間以上
- コールセンターの独自対応はよかった

---

- 「積極的に声をあげることでかえって責任問題となるリスクをとるよりも、自らの持ち場でやれることをやっていたといえるための行動をとる方が、組織内の行動として合理的な選択になってしまう」企業風土が原因と指摘される
- バッチ処理の時間オーバーに関しては心配がない状態だったが、警戒していなかった落とし穴にハマった
- 実機で試した更新は 8 万件、予定されていた更新は 70 万件超でテストが不十分
- MINORI は疎結合のアーキテクチャを採用していたが、蜜結合に向いたグローバルトランザクションという手法を採用していた（三井住友銀行では蜜結合なサブシステムは同一メインフレーム上で稼働している）
- 想定していない障害で、本番ぶっつけ対応を行った（成功した箇所と失敗した箇所がある）

---

- IT インフラは 5 年で刷新が一般的
- みずほは 10 年程度を見ていた
- ミッションクリティカルなシステムでは SSD が主流になっているが、みずほは HDD を使っていた

---

## 金融庁の分析

- コンプライアンスに関する知識が欠如
- 品質を確保するための検証が不足
- 運用体制が未整備
- 訓練や研修の不足
- 安定稼働と誤認、設計（アーキテクチャ）を過信
- 運用フェーズで人員を 67%削減
- 専門性のない CIO を任命
- 機能しないリスク委員会・監査委員会

---

P.178 について
2/28 の障害に対して、コールセンターは事務企画部の担当者と個別に相談し、臨時対応として顧客への案内を行った。（上からの指示があるより先に）
この対応は素晴らしいものだと考えられるが、本書では「営業店を統括するリテール・事業法人推進部やコーポレートコミュニケート部に働きかけていればもっと状況は良くなったかもしれない」＝「自らの持ち場でやれることだけに専念していた行動」と断じている。

これには違和感を覚える。
おそらく、かなり顧客対応に追われる中、上からの指示がない状態で個別調整して顧客のために対応したと思われるのに、この分析はいかがなものかと感じる。

---

## みずほ銀行システムの歴史

- 合併前から抱える、富士通・日本 IBM・日立のベンダー問題
- 1980 年代までは間接金融が中心で、融資のための資金集めとして預金獲得・口座獲得が重要だった
- 資金調達が直接金融（株式、債券発行）になるとリテールの収益性は下がった
- 2000 年代、地銀はシステムを共同化する動きがあったが、メガバンクは共同化にはあまりにも規模が大きすぎた
- MINORI では勘定系を複数のコンポーネント（サブシステム）に分割し、マルチベンダー体制をとった（上記 3 社 + NTT データ）
- コードを自動生成する「超高速開発ツール」を採用

---

### MINORI の特徴

- 1980 年代のシステム制約から解放
- 耐障害性を高める SOA
- バッチ処理の解体（バッチ方式 → オンライン方式）
- 要件定義から新規構築（つぎはぎではない）
- メインフレーム、COBOL、RDB を採用（当時、勘定システムのオープン化は難航していた）

---

- みずほは運用部門の力が弱い
- 運用が難しい（構成がバラバラ）なマルチベンダー体制が採用されてしまった
- 戦略的に MINORI 経験者を他プロジェクトに異動
- 運用子会社を IBM 傘下に移譲
- 結果として、運用体制の構築が手薄となった

---

## みずほ銀行でだけ障害が起きる理由

- UFJ 銀行も統合直後に障害が起きており、みずほに限った話ではなかった
- 2011 年の震災で発生した 1 週間近く夜間バッチが完了しなかった障害は日本では他に聞かないが海外では類似の事例がある
- 両行とも、夜間バッチの異常終了から翌営業日の強行営業によってデータの不整合が発生した
- 第三次オンラインの刷新は他行でも苦労していた（さくら銀行では頓挫）
- 連続障害は特異
- 他行は障害訓練やデータセンター切替運用を行なう
- 他行はシステム運用に力を入れていたが、みずほは開発のみに注力していた

---

## みずほ銀行は立ち直れるのか

- 追加研修や組織の見直し
- 113 項の再発防止策
- 部門横断の組織再編
- 顧客影響を念頭に置いた対策
- IT 有識者の経営登用
- 障害を語り継ぐ試み
- ミドルウェアの統一
- SRE の実践を目指す
